norm_param,
alpha,
B,
working_dir,
q=2,
eta=NULL,
cov_cor=NULL,
t_vector=NULL,
loud=NULL,
parallel_cores=NULL,
full_output=NULL,
norm_output=NULL,
svd_error_resolve=NULL,
segment_B_adjust=NULL){
# This function runs a binary segmentation algorithm to detect multiple
# changepoints
# INPUT
# x_matrix          - a pxn data matrix
# norm_param        - a single parameter, which is associated with the following
#                   -       0<norm_param<1     - use Frobenius norm + Ky-Fan norm
#                                                corresponding to top K singular values
#                                                representing norm_param% of the total sum
#                           norm_param=INTEGER - only use Ky-Fan(norm_param) norm
#                           norm_param='F'     - only use Frobenius norm (squared)
# alpha             - overall Type I error
# B                 - B number of permutations
# working_dir       - working directory
# q                 - parameter defining how many observations we ignore in maximum at start
#                     and end of time series, default is 2 (consider max over i=2,...,n-2)
# eta               - option to delete observations before and after CP
# cov_cor           - are we using covariance or correlation
# t_vector          - vector of values of t which we use in the testing
# loud              - whether to print messages while running
# parallel_cores    - optional argument; parallelizes calculation of test statistics for
#                     permutations of data
# svd_error_resolve - pass to CP detect single segment, sometimes helpful for large p
# segment_B_adjust  - set to TRUE if want to make sure at least 1/alpha permutations
#                     for each segment, if B>(1/alpha) than just use B
# OUTPUT
# changepoint_list  - a list
#                   - [[1]] 1 if any CP detected, 0 otw
#                     [[2]] changepoint_df - a dataframe of cp location and norm, ordered by
#                     when they were detected
#                     [[3]] a long list
#                     [[3]][[j]][[1]] - for the j-th segment examined, what the start and
#                                       end points of the segment are
#                     [[3]][[j]][[2]] - the output from run_permutation_test_single_segment
#                                       with norm_output an full_output options provided, if
#                                       norm_output and full_output are both equal to null this
#                                       is not provided
p=dim(x_matrix)[1]
n=dim(x_matrix)[2]
if(is.null(eta)){
eta=0
}
if(is.null(t_vector)){
t_vector=q:(n-q)
}
if(is.null(cov_cor)){
cov_cor='cov'
}
intervals_to_test=list(c(1,n))
changepoint_df=data.frame(location=numeric(),norm=character())
output_list=list()       # this contains everything that is ultimately outputted by this function
output_list[[3]]=list()  # this is the list of all the test stats from each segment
segment_count=0          # this keeps track of how many segments we have examined
while(length(intervals_to_test)>0){
start_index=intervals_to_test[[1]][1]
end_index=intervals_to_test[[1]][2]
segment_alpha=alpha*((end_index-start_index+1)/n)
segment_x_matrix=x_matrix[,start_index:end_index]
segment_count=segment_count+1
# Delete what we are testing out of intervals to test
intervals_to_test=intervals_to_test[-1]
if(!is.null(loud)){print(paste('Testing segment',start_index,end_index,segment_alpha))}
# Determine what times we are testing
segment_t_vector=intersect(start_index:end_index,t_vector)
segment_t_vector=intersect((start_index+(q-1)):(end_index-q),segment_t_vector)
# Adjust segment_t_vector from the start_index:end_index scale to the
# 1:(end_index-start_index+1) scale
segment_t_vector=segment_t_vector-start_index+1
# This is the case where there are not enough observations to be able to
# reject the null at segment_alpha level of significance
# For example if there are only 5 observations -> 5!=120 total permutations
# But if we wanted to use level of significance of alpha=0.005 we would need
# at least 1/.005=200 permutations
if((1/segment_alpha)>factorial(end_index-start_index+1)){
cp_test=list(0,0,0)
}else if((start_index+(q-1))>(end_index-q)){
# Here is case where because of q there are no time points to test
cp_test=list(0,0,0)
}else{
if(is.null(segment_B_adjust)){
segment_B=B
}else if(segment_B_adjust==TRUE){
segment_B=max(ceiling(1/segment_alpha),B)
}else{
segment_B=B
}
if(is.null(parallel_cores)){
cp_test=run_permutation_test_single_segment(segment_x_matrix,norm_param,segment_alpha,segment_B,working_dir,
cov_cor=cov_cor,t_vector=segment_t_vector,
full_output=full_output,norm_output=norm_output)
}else if(is.null(svd_error_resolve)){
cp_test=run_permutation_test_single_segment(segment_x_matrix,norm_param,segment_alpha,segment_B,working_dir,
cov_cor=cov_cor,t_vector=segment_t_vector,
parallel_cores = parallel_cores,
full_output=full_output,norm_output=norm_output,
svd_error_resolve = svd_error_resolve)
}else{
cp_test=run_permutation_test_single_segment(segment_x_matrix,norm_param,segment_alpha,segment_B,working_dir,
cov_cor=cov_cor,t_vector=segment_t_vector,
parallel_cores = parallel_cores,
full_output=full_output,norm_output=norm_output)
}
}
if(is.null(norm_output)&is.null(full_output)){
# Do nothing
}else{
output_list[[3]][[segment_count]]=list()
output_list[[3]][[segment_count]][[1]]=c(start_index,end_index)
output_list[[3]][[segment_count]][[2]]=cp_test
}
if(cp_test[[1]]==0){
if(!is.null(loud)){print('No CP found')}
}else{
cp_location=start_index+cp_test[[2]]-1
if(!is.null(loud)){print(paste('CP found at',cp_location))}
changepoint_df=rbind(changepoint_df,data.frame(location=cp_location,norm=cp_test[[3]]))
# Add new intervals
left_interval_start=start_index
left_interval_end=cp_location-eta
right_interval_start=cp_location+1+eta
right_interval_end=end_index
# Determine new intervals to test (if they are too small don't add)
if(left_interval_start<(left_interval_end-3)){
intervals_to_test=c(intervals_to_test,list(c(left_interval_start,left_interval_end)))
}
if(right_interval_start<(right_interval_end-3)){
intervals_to_test=c(intervals_to_test,list(c(right_interval_start,right_interval_end)))
}
}
}
if(is.null(norm_output)&is.null(full_output)){
# If don't care about norm or full output just return changepoint df
return(changepoint_df)
}else{
if(nrow(changepoint_df)>0){
output_list[[1]]=1
output_list[[2]]=changepoint_df
}else{
output_list[[1]]=0
}
return(output_list)
}
}
calculate_test_statistic_matrix<-function(T_matrix,x_matrix,norm_vec,t_vector,cov_cor){
# This helper function calculates a matrix of test statistics, for predefined
# norms and values of t
# INPUT
# T_matrix          - the matrix of test statistics we wish to fill out
# x_matrix          - multivariate time series
# norm_vec          - a vector of norms; numbers correspond to the Ky-Fan(k) norm, and F corresponds
#                     to the squared Frobenius norm
# cov_cor           - optional argument, either 'cov' or 'cor' which dictates if we take differences
#                     of covariance or correlation matrices
# OUTPUT
# T_matrix          - a matrix, where the number of columns corresponds to difference values of t
#                     and rows different norm values
# NOTE
# I suppress warnings at the calculation of svd step because it gives a
# warning when you request all the singular values (which I obviously dont want)
n=ncol(x_matrix)
p=nrow(x_matrix)
# Determine maximum singular value we need to calculate
if(length(norm_vec)>1){
max_singular_value=as.numeric(norm_vec[length(norm_vec)-1])
}else{
if(norm_vec=='F'){
max_singular_value='F'
}else{
max_singular_value=as.numeric(norm_vec)
}
}
t_vector=as.numeric(colnames(T_matrix))
for(t in t_vector){
# Calculate difference in sample covariance matrices
if(cov_cor=='cov'){
sample_diff=cov(t(x_matrix[,1:t]))-cov(t(x_matrix[,(t+1):n]))
}else if(cov_cor=='cor'){
sample_diff=cor(t(x_matrix[,1:t]))-cor(t(x_matrix[,(t+1):n]))
}
# Create a list of SVDs associated with each unique block
svd_list=list()
if(max_singular_value=='F'){
# No SVD since only using Frobenius
}else{
singular_values=sort(suppressWarnings(svds(sample_diff,k=max_singular_value,nu=0,nv=0))$d,decreasing=TRUE)
}
for(i in 1:nrow(T_matrix)){
single_norm=norm_vec[i]
if(single_norm=='F'){
T_matrix[i,as.character(t)]=norm(sample_diff,type='F')**2
}else{
T_matrix[i,as.character(t)]=sum(singular_values[1:as.numeric(norm_vec[i])])
}
}
}
return(T_matrix)
}
x=1
x=5
# Create variables from row of experiment_df
list2env(as.list(experiment_df[x,]), envir = .GlobalEnv)
setwd(WORKING_DIR)
source('NewAppsHelper.R')
set.seed(initial_seed+i)
cov_mats=generate_covariance_matrix(cov_structure,snr,p,cov_rank)
if(null_case == TRUE){
cov_matrix_1 = cov_mats[[1]]
cov_matrix_2 = cov_mats[[1]]
}else{
cov_matrix_1=cov_mats[[1]]
cov_matrix_2=cov_mats[[2]]
}
image(Matrix(cov_matrix_1),lwd=0)
library(Matrix)
image(Matrix(cov_matrix_1),lwd=0)
image(Matrix(cov_matrix_2),lwd=0)
first_half_n=round(n/2)
second_half_n=n-first_half_n
X_1=t(MASS::mvrnorm(n=first_half_n,mu=rep(0,p),Sigm=cov_matrix_1,tol=10))
X_2=t(MASS::mvrnorm(n=second_half_n,mu=rep(0,p),Sigm=cov_matrix_2,tol=10))
x_matrix=cbind(X_1,X_2)
group_1_residuals = x_matrix[,1:(n/2)]
group_2_residuals = x_matrix[,(n/2+1):(n)]
K = K_calculate(X_1,X_2,K_pct = 0.8,cov = TRUE)
K
snr
snr=3
cov_mats=generate_covariance_matrix(cov_structure,snr,p,cov_rank)
if(null_case == TRUE){
cov_matrix_1 = cov_mats[[1]]
cov_matrix_2 = cov_mats[[1]]
}else{
cov_matrix_1=cov_mats[[1]]
cov_matrix_2=cov_mats[[2]]
}
first_half_n=round(n/2)
second_half_n=n-first_half_n
X_1=t(MASS::mvrnorm(n=first_half_n,mu=rep(0,p),Sigm=cov_matrix_1,tol=10))
X_2=t(MASS::mvrnorm(n=second_half_n,mu=rep(0,p),Sigm=cov_matrix_2,tol=10))
K = K_calculate(X_1,X_2,K_pct = 0.8,cov = TRUE)
K
B=500
args(RACT)
seed
seed=NULL
is.null(seed)
!is.null(seed)
.Random.seed
help(.Random.seed)
set.seed(.Random.seed)
a=.Random.seed
rnorm(3)
set.seed(a)
rnorm(3)
#### RUN SIMULATION/ANALYSIS ####
setwd(WORKING_DIR)
WORKING_DIR="C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/NewApplications"
SPINS_PROCESSED_DIR = WORKING_DIR
SPINS_ORIG_DIR = file.path(WORKING_DIR,'SPINSex')
RESULTS_DIR=paste(WORKING_DIR,'/results',sep='')
FIGURE_DIR=paste(WORKING_DIR,'/figures',sep='')
#### RUN SIMULATION/ANALYSIS ####
setwd(WORKING_DIR)
source('RACT.R')
source('BrainHelperRevisions.R')
RACT_DIR = "C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/NewApplications/RACT"
#### RUN SIMULATION/ANALYSIS ####
setwd(RACT_DIR)
source('RACT.R')
RACT_DIR = "C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/NewApplications/RACT"
#### RUN SIMULATION/ANALYSIS ####
setwd(RACT_DIR)
source('RACT.R')
RACT_DIR = "C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/RACT"
#### RUN SIMULATION/ANALYSIS ####
setwd(RACT_DIR)
source('RACT.R')
RACT_DIR = "C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/RACT/R"
#### RUN SIMULATION/ANALYSIS ####
setwd(RACT_DIR)
source('RACT.R')
setwd(WORKING_DIR)
source('BrainHelperRevisions.R')
X_1=t(MASS::mvrnorm(n=first_half_n,mu=rep(0,p),Sigm=cov_matrix_1,tol=10))
X_2=t(MASS::mvrnorm(n=second_half_n,mu=rep(0,p),Sigm=cov_matrix_2,tol=10))
RACT_result_1 = RACT(X_1,X_2,n_perm = B, K = NULL,min_P=FALSE,cov=cov)
RACT_result_1
snr
snr=.015
first_half_n=round(n/2)
second_half_n=n-first_half_n
X_1=t(MASS::mvrnorm(n=first_half_n,mu=rep(0,p),Sigm=cov_matrix_1,tol=10))
X_2=t(MASS::mvrnorm(n=second_half_n,mu=rep(0,p),Sigm=cov_matrix_2,tol=10))
RACT_result_1 = RACT(X_1,X_2,n_perm = B, K = NULL,min_P=FALSE,cov=cov)
RACT_result_1
B
snr
n_perm
K
K=seq(1,20)
RACT_result_1 = RACT(X_1,X_2,n_perm = B, K = NULL,min_P=FALSE,cov=cov)
RACT_result_1
K
RACT_result_2 = RACT(X_1,X_2,n_perm = B, K = c(1,3,7),min_P=TRUE,cov=cov)
RACT_result_2
snr=0
snr=.015
cov_mats=generate_covariance_matrix(cov_structure,snr,p,cov_rank)
if(null_case == TRUE){
cov_matrix_1 = cov_mats[[1]]
cov_matrix_2 = cov_mats[[1]]
}else{
cov_matrix_1=cov_mats[[1]]
cov_matrix_2=cov_mats[[2]]
}
first_half_n=round(n/2)
second_half_n=n-first_half_n
X_1=t(MASS::mvrnorm(n=first_half_n,mu=rep(0,p),Sigm=cov_matrix_1,tol=10))
X_2=t(MASS::mvrnorm(n=second_half_n,mu=rep(0,p),Sigm=cov_matrix_2,tol=10))
RACT_result_1 = RACT(X_1,X_2,n_perm = B, K = NULL,min_P=FALSE,cov=cov)
RACT_result_1
RESULTS_DIR=paste(RACT_DIR,'/results',sep='')
setwd(RESULTS_DIR)
RESULTS_DIR
RESULTS_DIR='C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/RACT_test/results'
setwd(RESULTS_DIR)
list.files()
result = readRDS(file.path(RESULTS_DIR,'simulated_power','simulated_power-001.RDS'))
result
experiment_df
result = readRDS(file.path(RESULTS_DIR,'RACT_test_power','RACT_tesT_power-001.RDS'))
result
experiment_df[1]
experiment_df[1,]
unlist(result)
result[[1]]
RACT_p = sapply(result, function(x) x[[1]][['RACT p value']])
RACT_p
hist(RACT_p)
hist(RACT_p,breaks=100)
result[[1]]
result[[2]]
document()
library(devtools)
document()
getwd()
setwd('C:/Users/davev/Documents/UofT/PhD/Research/Brain/BrainR/RACT')
document()
document()
document()
if (!require("devtools"))
install.packages("devtools")
devtools::install_github("daveveitch/RACT")
detach("package:RACT", unload=TRUE)
if (!require("devtools"))
install.packages("devtools")
devtools::install_github("daveveitch/RACT")
library(RACT)
help(RACT)
document()
n = 50
p = 250
snr = .02
set.seed(1)
X_1 = matrix(rnorm(n*p),nrow=p,ncol=n)
cov_X_2 = diag(p)*(1-snr)+snr
svd_cov_X_2 = svd(cov_X_2)
A = svd_cov_X_2$u%*%diag(sqrt(svd_cov_X_2$d))%*%t(svd_cov_X_2$v)
X_2 = A%*%matrix(rnorm(n*p),nrow=p,ncol=n)
RACT(X_1,X_2)
library(RACT)
help(RACT)
RACT(X_1,X_2)
RACT()
RACT(X_1)
RACT(X_1,X_2)
X_1
detach('package::RACT',unload=TRUE)
detach('RACT',unload=TRUE)
detach(RACT,unload=TRUE)
detach(package::RACT,unload=TRUE)
#'   Ky-Fan(k) norms that are included. E.g. K=c(1,5) means RACT is based on the Ky-Fan(1) and Ky-Fan(5) norms only.
#' @param min_P Whether a minimum-P value statistic is used.
#' @param cov If cov=TRUE RACT tests for the equality of the covariance of X_1,X_2. If cov=FALSE RACT tests for the equality of the correlation of X_1,X_2.
#' @param seed Allows user to set a seed for the permutation procedure.
#' @return a list of length 2. 'adaptive_p_value' is the p-value associated with RACT's adaptive test statistic. 'p_value_vector' a vector with p-values
#'   associated with individual Ky-Fan(k) norms
#' @export
#'
#' @examples
#' X_1 =
RACT <- function(X_1,X_2,n_perm=1000,K=NULL,min_P=FALSE,cov=TRUE,seed=NULL){
# Calculate covariance/correlation matrices for each group
if(cov == TRUE){
cov_cor_X_1 = cov(t(X_1))
cov_cor_X_2 = cov(t(X_2))
}else if(cov == FALSE){
cov_cor_X_1 = cor(t(X_1))
cov_cor_X_2 = cor(t(X_2))
}else{
stop('cov must be set to TRUE/FALSE')
}
if(is.null(K)){
K = K_calculate(X_1,X_2, cov = cov)
}else if(!all(K==floor(K))){
# Checks that all entries of K are integers
stop('K must be a vector of integers')
}else if(min(K)<=0){
stop('Each entry of K must be greater than or equal to 1')
}else if(max(K)>=nrow(cov_cor_X_1)){
stop('Each enry of K must be less than or equal to p')
}
if(!is.null(seed)){
# save current seed, will return to this seed after permutations done
old_seed = .Random.seed
set.seed(seed)
}
# Ensure all entries of K unique
K = unique(K)
observed_test_stats = calc_ky_fan_k(cov_cor_X_1,cov_cor_X_2,K)
# Create empty matrix of permutation statistics we will fill
permutation_stat_matrix = matrix(data=0,nrow=n_perm,ncol=length(observed_test_stats))
colnames(permutation_stat_matrix) = names(observed_test_stats)
# Permute data and record test statistics using permuted data in permutation_stat_matrix
X_1_X_2 = cbind(X_1,X_2)
for(j in 1:n_perm){
# Randomly permute the data
permuted_order = sample(1:ncol(X_1_X_2))
permuted_group_1 = X_1_X_2[,permuted_order[1:ncol(X_1)]]
permuted_group_2 = X_1_X_2[,permuted_order[(ncol(X_1)+1):ncol(X_1_X_2)]]
# Calculate test statistics for permuted data, and add to permutation_stat_matrix
if(cov == TRUE){
cov_cor_permuted_group_1 = cov(t(permuted_group_1))
cov_cor_permuted_group_2 = cov(t(permuted_group_2))
}else{
cov_cor_permuted_group_1 = cor(t(permuted_group_1))
cov_cor_permuted_group_2 = cor(t(permuted_group_2))
}
permuted_test_stat = calc_ky_fan_k(cov_cor_permuted_group_1,cov_cor_permuted_group_2,K)
permutation_stat_matrix[j,] = c(permuted_test_stat)
}
if(min_P == TRUE){
# Calculate p values for statistics in each permutation, and then take minimum p across these statistics to get
# permutation distribution of minimum p value. Note here we do not need to normalize the statistics since the same
# normalization is applied to every Ky-Fan(k) norm so this will not affect the p-value.
permutation_p_value_matrix = (n_perm-apply(permutation_stat_matrix,2,rank,ties.method='min')+1)/n_perm
# Calculate observed p values
observed_p_values = (colSums(t(t(permutation_stat_matrix)>=observed_test_stats))+1)/(n_perm+1)
# Add minimum p value column to permutation value matrix and calculate p-value of minimum p-value from observed data
permutation_min_p_values = apply(permutation_p_value_matrix,1,min)
observed_min_p_value = min(observed_p_values)
RACT_p_value = (sum(permutation_min_p_values <= observed_min_p_value) + 1)/(n_perm + 1)
}else{
# Below we take a max T approach. First normalize observed test statistics, and permutation test statistics
# by means and variances estimated via permutation
test_stat_means = apply(permutation_stat_matrix,2,mean)
test_stat_sd = apply(permutation_stat_matrix,2,sd)
normalized_observed_test_stats = (observed_test_stats - test_stat_means)/test_stat_sd
normalized_permutation_stat_matrix = sweep(permutation_stat_matrix,2,test_stat_means,'-')
normalized_permutation_stat_matrix = sweep(normalized_permutation_stat_matrix,2,test_stat_sd,'/')
# Calculate observed p values and RACT p value
observed_p_values = ((colSums(t(t(normalized_permutation_stat_matrix)>=normalized_observed_test_stats))+1)/(n_perm+1))
max_normalized_permutation_stats = apply(normalized_permutation_stat_matrix,1,max)
RACT_p_value = (sum(max_normalized_permutation_stats>=max(normalized_observed_test_stats))+1)/(n_perm+1)
}
if(!is.null(seed)){
set.seed(old_seed)
}
return(list('RACT p value'=RACT_p_value, 'Individual Ky-Fan(k) p values'= observed_p_values))
}
RACT(X_1,X_2)
#' Calculate Ky-Fan(k) norms
#'
#' @description This function takes in two covariance/correlation matrices, and calculates the Ky-Fan(k) norm of their differences for k=1,...,K
#'
#' @param cov_cor_X_1 p x p (p dimensions) covariance or correlation matrix for group 1
#' @param cov_cor_X_2 p x p  (p dimensions) covariance or correlation matrix for group 2
#' @param K Vector of integers, and these integers represent the Ky-Fan(k) norms that are included.
#'   E.g. K=c(1,5) means RACT is based on the Ky-Fan(1) and Ky-Fan(5) norm.
#' @return named vector of Ky-Fan(k) norms where names of each entry are values of k
#' @noRd
calc_ky_fan_k<-function(cov_cor_X_1,cov_cor_X_2,K){
diff_mat = cov_cor_X_1-cov_cor_X_2
# Calculate top K singular values of difference matrix
top_K_singular_values = sort(RSpectra::svds(diff_mat,k=max(K),nu=0,nv=0)$d,decreasing=TRUE)
ky_fan_k_norms = cumsum(top_K_singular_values)[K]
names(ky_fan_k_norms) = K
return(ky_fan_k_norms)
}
RACT(X_1,X_2)
RACT(X_1,X_2,n_perm=200)
RACT(X_1,X_2,n_perm=200,seed=1)
RACT(X_1,X_2,n_perm=200,seed=2)
document()
library(devtools)
document()
library(devtools)
document()
document()
