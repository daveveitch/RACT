# Dependencies: foreach, doparallel, RSpectra

#### Generate Simulated Data ####
generate_covariance_matrix<-function(cov_structure,snr,p,cov_rank,orig_cov_matrix=NULL){
  # This function generates two matries based on one of the
  # several covariance structures laid out in the paper
  # INPUT
  # cov_structure           - number related to which scenario we are using
  # cov_rank                - rank of the low rank component
  # snr                     - equivalent to tau^2 in Sigma=I+tau^2 UU^T
  # p                       - dimension
  # orig_cov_matrix         - only really used for scenario 5 where a change occurs
  #                           in off diagonals and diagonals kept constant
  # OUTPUT
  # cov_matrix              - a covariance matrix
  
  cov_matrix_1=matrix(data=0,nrow=p,ncol=p)
  cov_matrix_2=matrix(data=0,nrow=p,ncol=p)
  
  # Low Rank Component
  if(cov_structure=='(LowRank)'){
    b=matrix(rnorm(p*cov_rank),ncol=cov_rank)
    cov_matrix_1=snr*(b%*%t(b))
    
    b=matrix(rnorm(p*cov_rank),ncol=cov_rank)
    cov_matrix_2=snr*(b%*%t(b))
    
  }else if(cov_structure=='(LowRankBlockLarge)'){
    block_size=round(p/2)
    
    b=matrix(rnorm(block_size*cov_rank),ncol=cov_rank)
    cov_matrix_1[1:block_size,1:block_size]=snr*(b%*%t(b))
    
    b=matrix(rnorm((p-block_size)*cov_rank),ncol=cov_rank)
    cov_matrix_1[(block_size+1):p,(block_size+1):p]=b%*%t(b)
    
    b=matrix(rnorm(block_size*cov_rank),ncol=cov_rank)
    cov_matrix_2[1:block_size,1:block_size]=snr*(b%*%t(b))
    
    cov_matrix_2[(block_size+1):p,(block_size+1):p]=cov_matrix_1[(block_size+1):p,(block_size+1):p]
    
  }else if(cov_structure=='(LowRankBlockSmall)'){
    block_size=10
    
    b=matrix(rnorm(block_size*cov_rank),ncol=cov_rank)
    cov_matrix_1[1:block_size,1:block_size]=snr*(b%*%t(b))
    
    b=matrix(rnorm((p-block_size)*cov_rank),ncol=cov_rank)
    cov_matrix_1[(block_size+1):p,(block_size+1):p]=b%*%t(b)
    
    b=matrix(rnorm(block_size*cov_rank),ncol=cov_rank)
    cov_matrix_2[1:block_size,1:block_size]=snr*(b%*%t(b))
    
    cov_matrix_2[(block_size+1):p,(block_size+1):p]=cov_matrix_1[(block_size+1):p,(block_size+1):p]
    
  }else if(cov_structure=='(OffDiagonal)'){
    block_size=round(p/2)
    delete_block_size=ceiling(p/4-.00000001)
    
    b=matrix(rnorm(block_size*cov_rank),ncol=cov_rank)
    cov_matrix_1[1:block_size,1:block_size]=snr
    
    cov_matrix_2=cov_matrix_1
    cov_matrix_2[(delete_block_size+1):block_size,1:delete_block_size]=-1*snr
    cov_matrix_2[1:delete_block_size,(delete_block_size+1):block_size]=-1*snr
    
    diag(cov_matrix_1)=0
    diag(cov_matrix_2)=0
    
  }
  
  # Add diagonals
  cov_matrix_1=cov_matrix_1+diag(rep(1,p))
  cov_matrix_2=cov_matrix_2+diag(rep(1,p))
  
  return(list(cov_matrix_1,cov_matrix_2)) 
  
}

generate_covariance_matrix_2CP<-function(cov_structure,snr,p,cov_rank,orig_cov_matrix=NULL){
  # This function generates three covariance matrices for the 2CP case, i.e. where you only want
  # the portion you care about to be changing
  # This function generates two matries based on one of the
  # several covariance structures laid out in the paper
  # INPUT
  # cov_structure           - number related to which scenario we are using
  # cov_rank                - rank of the low rank component
  # snr                     - equivalent to tau^2 in Sigma=I+tau^2 UU^T
  # p                       - dimension
  # orig_cov_matrix         - only really used for scenario 5 where a change occurs
  #                           in off diagonals and diagonals kept constant
  # OUTPUT
  # cov_matrix_list         - a list of 3 covariance matrices
  
  cov_matrix_list=list()
  
  if(cov_structure=='(LowRank)'){
    cov_matrix_1=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    cov_matrix_2=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    cov_matrix_3=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
  }else if(cov_structure=='(LowRankBlockLarge)'){
    block_size=round(p/2)
    
    cov_matrix_1=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    cov_matrix_2=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    cov_matrix_3=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    
    cov_matrix_2[(block_size+1):p,(block_size+1):p]=cov_matrix_1[(block_size+1):p,(block_size+1):p]
    cov_matrix_3[(block_size+1):p,(block_size+1):p]=cov_matrix_1[(block_size+1):p,(block_size+1):p]
    
  }else if(cov_structure=='(LowRankBlockSmall)'){
    block_size=2*cov_rank
    
    cov_matrix_1=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    cov_matrix_2=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    cov_matrix_3=generate_covariance_matrix(cov_structure,snr,p,cov_rank)[[1]]
    
    cov_matrix_2[(block_size+1):p,(block_size+1):p]=cov_matrix_1[(block_size+1):p,(block_size+1):p]
    cov_matrix_3[(block_size+1):p,(block_size+1):p]=cov_matrix_1[(block_size+1):p,(block_size+1):p]
    
  }else if(cov_structure=='(OffDiagonal)'){
    cov_matrix_1_2=generate_covariance_matrix(cov_structure,snr,p,cov_rank)
    cov_matrix_1=cov_matrix_1_2[[1]]
    cov_matrix_2=cov_matrix_1_2[[2]]
    cov_matrix_3=cov_matrix_1
  }
  
  cov_matrix_list[[1]]=cov_matrix_1
  cov_matrix_list[[2]]=cov_matrix_2
  cov_matrix_list[[3]]=cov_matrix_3
  
  return(cov_matrix_list)
}

#### Permutation Tests ####
run_permutation_test_single_segment<-function(x_matrix,
                                              norm_param,
                                              alpha,
                                              B,
                                              working_dir,
                                              q=2,
                                              cov_cor=NULL,
                                              full_output=NULL,
                                              norm_output=NULL,
                                              t_vector=NULL,
                                              parallel_cores=NULL,
                                              svd_error_resolve=NULL,
                                              segment_B_adjust=NULL){
  # This function runs a permutation test on a single segment of data
  # level alpha
  # INPUT
  # x_matrix          - pxn matrix of data
  # norm_param        - a single parameter, which is associated with the following
  #                   -       0<norm_param<1     - use Frobenius norm + Ky-Fan norm
  #                                                corresponding to top K singular values
  #                                                representing norm_param% of the total sum
  #                           norm_param=INTEGER - only use Ky-Fan(norm_param) norm
  #                           norm_param='F'     - only use Frobenius norm (squared)
  # alpha             - size of the test
  # working_dir       - working directory
  # B                 - number of permutations, note if B=0 then this function
  # q                 - how many observations at beginning and end to ignore for maximum
  #                     just returns the test statistic matrix
  # cov_cor           - an optional argument, default is cov, but if user specifies cor
  #                     then we calculate correlations not covariances
  # full_output       - if TRUE then we return test statistic array and permutation array
  # norm_output       - here method will only return the unnormalized test statistics, the
  #                     mean and standard deviation matrix of the permutations, and the
  #                     cutoff value of whether to accept/reject the null
  # t_vector          - optional argument, if non null, contains times you would like
  #                     to calculate a test statistic for
  # svd_error_resolve - there were simulations where the full SVD of a very large p matrix (this is
  #                     done when determining how many singular values to use for RACC)
  #                     with several very large singular values and many small ones was throwing an 
  #                     error 'La.svd(x, nu, nv) : error code 1 from Lapack routine 'dgesdd'
  #                     to get around this error we calculate p-1 singular values instead
  # OUTPUT
  # changepoint_vector - list of length 3 (if full_output not selected, 4 otherwise)
  #                      [[1]] - 1 if CP detected, 0 if not
  #                      [[2]] - location of changepoint
  #                      [[3]] - what norm was associated with the rejection
  #                      [[4]] - list of length 2 [[1]] test statistic array [[2]] permutation array
  #                      [[5]] - list of length 3 [[1]] unnormalized test statistics
  #                                               [[2]] mean matrix
  #                                               [[3]] stdev matrix
  #                                               [[4]] maximum values from each permutation
  
  n=ncol(x_matrix) # length of segment
  p=nrow(x_matrix)
  
  if(is.null(t_vector)){
    t_vector=q:(n-q)
  }
  if(is.null(cov_cor)){
    cov_cor='cov'
  }
  
  if(!is.null(segment_B_adjust)){
    if(segment_B_adjust==TRUE){
      B=max(ceiling(1/alpha),B)
    }
  }
  
  # Determine norms you will use
  norm_vec=c()
  
  if(norm_param=='F'){
    norm_vec=norm_param
  }else if(as.numeric(norm_param)>=1){
    norm_vec=as.numeric(norm_param)
  }else if((as.numeric(norm_param)>0)&(as.numeric(norm_param)<1)){
    # Case where you choose singular values representing top norm_param of sum
    if(cov_cor=='cov'){
      whole_segment_cov=cov(t(x_matrix))  
    }else{
      whole_segment_cov=cor(t(x_matrix))
    }
    
    if(is.null(svd_error_resolve)){
      whole_segment_singular_values=svd(whole_segment_cov)$d  
    }else if(svd_error_resolve==TRUE){
      whole_segment_singular_values=svds(whole_segment_cov,k=p-1)$d  
    }
    
    cumsum_singular_values=cumsum(whole_segment_singular_values)
    max_singular_value=min(which(cumsum_singular_values>(as.numeric(norm_param)*sum(whole_segment_singular_values))))
    
    norm_vec=c(seq(1,max_singular_value),'F')
  }
  
  # Create matrix of test statistics of original data
  T_matrix=matrix(data=NA,nrow=length(norm_vec),ncol=length(t_vector),
                  dimnames=list(norm_vec,t_vector))
  
  # Calculate test statistics
  T_matrix=calculate_test_statistic_matrix(T_matrix,x_matrix,norm_vec,t_vector,cov_cor)
  
  # If all you want is the test statistic matrix, then set B=0 and you get it
  if(B==0){
    return(T_matrix)
  }
  
  # Create arrays to store statistics for different permutations
  permuted_T_array=array(data=NA,c(length(norm_vec),length(t_vector),B),
                         dimnames=list(norm_vec,t_vector,1:B))
  
  # Create matrix of mean and standard deviations of permutation statistics (used to normalize)
  mean_matrix=matrix(data=NA,nrow=length(norm_vec),ncol=length(t_vector),
                     dimnames=list(norm_vec,t_vector))
  sd_matrix=matrix(data=NA,nrow=length(norm_vec),ncol=length(t_vector),
                   dimnames=list(norm_vec,t_vector))
  
  # Permutation distribution of max test statistic
  permutation_distribution_T_max=rep(0,B)   
  
  # Calculate Permutation distribution
  permutation_seeds=sample(1:1000000,B,replace=FALSE)
  if(is.null(parallel_cores)){
    for(b in 1:B){
      # Permute time points
      set.seed(permutation_seeds[b])
      permuted_x_matrix=x_matrix[,sample(1:n)]
      permuted_T_matrix=matrix(data=NA,nrow=length(norm_vec),ncol=length(t_vector),
                               dimnames=list(norm_vec,t_vector))
      permuted_T_matrix=calculate_test_statistic_matrix(permuted_T_matrix,permuted_x_matrix,norm_vec,t_vector,cov_cor)
      permuted_T_array[,,b]=permuted_T_matrix
    }  
  }else{
    cl_permutation <- parallel::makeCluster(parallel_cores,outfile="")
    doParallel::registerDoParallel(cl_permutation)
    setwd(working_dir)
    
    permutation_T_matrices=foreach(b=1:B, 
                               .packages = c('RSpectra'),.combine='c')%dopar%{
                                 
                                 setwd(working_dir)
                                 source('BrainHelperRevisions.R')
                                 
                                 set.seed(permutation_seeds[b])
                                 permuted_x_matrix=x_matrix[,sample(1:n)]
                                 permuted_T_matrix=matrix(data=NA,nrow=length(norm_vec),ncol=length(t_vector),
                                                          dimnames=list(norm_vec,t_vector))
                                 permuted_T_matrix=calculate_test_statistic_matrix(permuted_T_matrix,permuted_x_matrix,norm_vec,t_vector,cov_cor)
                                 
                                 if((b%%100)==0){print(paste(b,Sys.time()))}
                                 
                                 list(permuted_T_matrix)
                               }
    
    parallel::stopCluster(cl_permutation)
    
    permuted_T_array=array(unlist(permutation_T_matrices),
                           dim=c(nrow(T_matrix),ncol(T_matrix),B),
                           dimnames=list(norm_vec,t_vector,1:B))
    
  }
  
  # Calculate mean and standard deviation of boostrap statistics
  mean_matrix=apply(permuted_T_array,c(1,2),mean)
  sd_matrix=apply(permuted_T_array,c(1,2),sd)

  # Normalize the T_matrix and permuted_T_array
  T_matrix_normalized=(T_matrix-mean_matrix)/sd_matrix
  permuted_T_array_normalized=permuted_T_array
  
  for(b in 1:B){
    permuted_T_array_normalized[,,b]=(permuted_T_array[,,b]-mean_matrix)/sd_matrix
    permutation_distribution_T_max[b]=max(permuted_T_array_normalized[,,b])
  }
  
  # Calculate critical value
  permutation_crit_value=quantile(permutation_distribution_T_max,1-alpha)
  
  # Create output of function
  changepoint_list=list(0,0,0)
  
  # [[1]] Existence of changepoint
  if(max(T_matrix_normalized)>permutation_crit_value){
    changepoint_list[[1]]=1
    # Row column of location (in case of ties just take first one)
    # ties only really happen when the number of samples are less than the rank of the matrix
    cp_loc=c(which(T_matrix_normalized==max(T_matrix_normalized),arr.ind=TRUE)[1,])
    changepoint_list[[2]]=as.numeric(colnames(T_matrix_normalized)[cp_loc[2]])
    changepoint_list[[3]]=(rownames(T_matrix_normalized)[cp_loc[1]])
  }
  
  if(!is.null(full_output)){
    if(full_output){
      changepoint_list[[4]]=list(T_matrix_normalized,permuted_T_array_normalized)
    }
  }
  
  if(!is.null(norm_output)){
    if(norm_output){
      norm_output_list=list()
      norm_output_list[[1]]=T_matrix
      norm_output_list[[2]]=mean_matrix
      norm_output_list[[3]]=sd_matrix
      norm_output_list[[4]]=permutation_distribution_T_max
      changepoint_list[[5]]=norm_output_list
    }
  }

  return(changepoint_list)
}

run_permutation_test_multi_segment<-function(x_matrix,
                                             norm_param,
                                             alpha,
                                             B,
                                             working_dir,
                                             q=2,
                                             eta=NULL,
                                             cov_cor=NULL,
                                             t_vector=NULL,
                                             loud=NULL,
                                             parallel_cores=NULL,
                                             full_output=NULL,
                                             norm_output=NULL,
                                             svd_error_resolve=NULL,
                                             segment_B_adjust=NULL){
  # This function runs a binary segmentation algorithm to detect multiple
  # changepoints
  # INPUT
  # x_matrix          - a pxn data matrix
  # norm_param        - a single parameter, which is associated with the following
  #                   -       0<norm_param<1     - use Frobenius norm + Ky-Fan norm
  #                                                corresponding to top K singular values
  #                                                representing norm_param% of the total sum
  #                           norm_param=INTEGER - only use Ky-Fan(norm_param) norm
  #                           norm_param='F'     - only use Frobenius norm (squared)
  # alpha             - overall Type I error
  # B                 - B number of permutations
  # working_dir       - working directory
  # q                 - parameter defining how many observations we ignore in maximum at start
  #                     and end of time series, default is 2 (consider max over i=2,...,n-2)
  # eta               - option to delete observations before and after CP
  # cov_cor           - are we using covariance or correlation
  # t_vector          - vector of values of t which we use in the testing
  # loud              - whether to print messages while running
  # parallel_cores    - optional argument; parallelizes calculation of test statistics for
  #                     permutations of data
  # svd_error_resolve - pass to CP detect single segment, sometimes helpful for large p
  # segment_B_adjust  - set to TRUE if want to make sure at least 1/alpha permutations
  #                     for each segment, if B>(1/alpha) than just use B
  # OUTPUT
  # changepoint_list  - a list
  #                   - [[1]] 1 if any CP detected, 0 otw
  #                     [[2]] changepoint_df - a dataframe of cp location and norm, ordered by
  #                     when they were detected
  #                     [[3]] a long list
  #                     [[3]][[j]][[1]] - for the j-th segment examined, what the start and
  #                                       end points of the segment are
  #                     [[3]][[j]][[2]] - the output from run_permutation_test_single_segment
  #                                       with norm_output an full_output options provided, if
  #                                       norm_output and full_output are both equal to null this
  #                                       is not provided
  
  p=dim(x_matrix)[1]
  n=dim(x_matrix)[2]  
  
  if(is.null(eta)){
    eta=0
  }
  
  if(is.null(t_vector)){
    t_vector=q:(n-q)
  }
  if(is.null(cov_cor)){
    cov_cor='cov'
  }
  
  intervals_to_test=list(c(1,n))
  
  changepoint_df=data.frame(location=numeric(),norm=character())
  
  output_list=list()       # this contains everything that is ultimately outputted by this function
  output_list[[3]]=list()  # this is the list of all the test stats from each segment
  segment_count=0          # this keeps track of how many segments we have examined
  
  while(length(intervals_to_test)>0){
    start_index=intervals_to_test[[1]][1]
    end_index=intervals_to_test[[1]][2]
    segment_alpha=alpha*((end_index-start_index+1)/n)
    segment_x_matrix=x_matrix[,start_index:end_index]
    segment_count=segment_count+1
    
    # Delete what we are testing out of intervals to test
    intervals_to_test=intervals_to_test[-1]
    
    if(!is.null(loud)){print(paste('Testing segment',start_index,end_index,segment_alpha))}
    
    # Determine what times we are testing
    segment_t_vector=intersect(start_index:end_index,t_vector)
    segment_t_vector=intersect((start_index+(q-1)):(end_index-q),segment_t_vector)
    
    # Adjust segment_t_vector from the start_index:end_index scale to the
    # 1:(end_index-start_index+1) scale
    segment_t_vector=segment_t_vector-start_index+1
    
    # This is the case where there are not enough observations to be able to
    # reject the null at segment_alpha level of significance
    # For example if there are only 5 observations -> 5!=120 total permutations
    # But if we wanted to use level of significance of alpha=0.005 we would need
    # at least 1/.005=200 permutations 
    if((1/segment_alpha)>factorial(end_index-start_index+1)){
      cp_test=list(0,0,0)
    }else if((start_index+(q-1))>(end_index-q)){
      # Here is case where because of q there are no time points to test
      cp_test=list(0,0,0)
    }else{
      
      if(is.null(segment_B_adjust)){
        segment_B=B
      }else if(segment_B_adjust==TRUE){
        segment_B=max(ceiling(1/segment_alpha),B)
      }else{
        segment_B=B
      }
      
      if(is.null(parallel_cores)){
        cp_test=run_permutation_test_single_segment(segment_x_matrix,norm_param,segment_alpha,segment_B,working_dir,
                                                    cov_cor=cov_cor,t_vector=segment_t_vector,
                                                    full_output=full_output,norm_output=norm_output)  
        
      }else if(is.null(svd_error_resolve)){
        cp_test=run_permutation_test_single_segment(segment_x_matrix,norm_param,segment_alpha,segment_B,working_dir,
                                                    cov_cor=cov_cor,t_vector=segment_t_vector,
                                                    parallel_cores = parallel_cores,
                                                    full_output=full_output,norm_output=norm_output,
                                                    svd_error_resolve = svd_error_resolve)
      }else{
        cp_test=run_permutation_test_single_segment(segment_x_matrix,norm_param,segment_alpha,segment_B,working_dir,
                                                    cov_cor=cov_cor,t_vector=segment_t_vector,
                                                    parallel_cores = parallel_cores,
                                                    full_output=full_output,norm_output=norm_output) 
      }  
    }
    
    if(is.null(norm_output)&is.null(full_output)){
      # Do nothing
    }else{
      output_list[[3]][[segment_count]]=list()
      output_list[[3]][[segment_count]][[1]]=c(start_index,end_index)
      output_list[[3]][[segment_count]][[2]]=cp_test
    }
    
    if(cp_test[[1]]==0){
      if(!is.null(loud)){print('No CP found')}
    }else{
      cp_location=start_index+cp_test[[2]]-1
      
      if(!is.null(loud)){print(paste('CP found at',cp_location))}
      
      changepoint_df=rbind(changepoint_df,data.frame(location=cp_location,norm=cp_test[[3]]))
   
      # Add new intervals
      left_interval_start=start_index
      left_interval_end=cp_location-eta
      right_interval_start=cp_location+1+eta
      right_interval_end=end_index
      
      # Determine new intervals to test (if they are too small don't add)
      if(left_interval_start<(left_interval_end-3)){
          intervals_to_test=c(intervals_to_test,list(c(left_interval_start,left_interval_end)))
      }
      if(right_interval_start<(right_interval_end-3)){
          intervals_to_test=c(intervals_to_test,list(c(right_interval_start,right_interval_end)))
      }
    }
  }
  
  if(is.null(norm_output)&is.null(full_output)){
    # If don't care about norm or full output just return changepoint df
    return(changepoint_df)
  }else{
    if(nrow(changepoint_df)>0){
      output_list[[1]]=1
      output_list[[2]]=changepoint_df
    }else{
      output_list[[1]]=0
    }
    
    return(output_list)
  }
}

calculate_test_statistic_matrix<-function(T_matrix,x_matrix,norm_vec,t_vector,cov_cor){
  # This helper function calculates a matrix of test statistics, for predefined
  # norms and values of t
  # INPUT
  # T_matrix          - the matrix of test statistics we wish to fill out
  # x_matrix          - multivariate time series
  # norm_vec          - a vector of norms; numbers correspond to the Ky-Fan(k) norm, and F corresponds
  #                     to the squared Frobenius norm
  # cov_cor           - optional argument, either 'cov' or 'cor' which dictates if we take differences
  #                     of covariance or correlation matrices
  # OUTPUT
  # T_matrix          - a matrix, where the number of columns corresponds to difference values of t
  #                     and rows different norm values
  # NOTE
  # I suppress warnings at the calculation of svd step because it gives a 
  # warning when you request all the singular values (which I obviously dont want)
  
  n=ncol(x_matrix)
  p=nrow(x_matrix)
  
  # Determine maximum singular value we need to calculate
  if(length(norm_vec)>1){
    max_singular_value=as.numeric(norm_vec[length(norm_vec)-1])
  }else{
    if(norm_vec=='F'){
      max_singular_value='F'
    }else{
      max_singular_value=as.numeric(norm_vec)
    }
  }
  
  t_vector=as.numeric(colnames(T_matrix))
  
  for(t in t_vector){
    # Calculate difference in sample covariance matrices
    if(cov_cor=='cov'){
      sample_diff=cov(t(x_matrix[,1:t]))-cov(t(x_matrix[,(t+1):n]))
    }else if(cov_cor=='cor'){
      sample_diff=cor(t(x_matrix[,1:t]))-cor(t(x_matrix[,(t+1):n]))
    }
    
    # Create a list of SVDs associated with each unique block
    svd_list=list()
    
    if(max_singular_value=='F'){
      # No SVD since only using Frobenius
    }else{
      singular_values=sort(suppressWarnings(svds(sample_diff,k=max_singular_value,nu=0,nv=0))$d,decreasing=TRUE)
    }
    
    for(i in 1:nrow(T_matrix)){
      single_norm=norm_vec[i]
      
      if(single_norm=='F'){
        T_matrix[i,as.character(t)]=norm(sample_diff,type='F')**2
      }else{
        T_matrix[i,as.character(t)]=sum(singular_values[1:as.numeric(norm_vec[i])])
      }
    }
  }
  
  return(T_matrix)
  
}

